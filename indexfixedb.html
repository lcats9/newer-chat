<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Claude Roleplay with Fixed Voice Detection</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }
        
        .container {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }
        
        h1 {
            text-align: center;
            color: #4a5568;
            margin-bottom: 30px;
        }
        
        .api-setup {
            background: #f7fafc;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            border-left: 4px solid #4299e1;
        }
        
        .input-group {
            margin-bottom: 15px;
        }
        
        label {
            display: block;
            margin-bottom: 5px;
            font-weight: 600;
            color: #2d3748;
        }
        
        input, textarea, select {
            width: 100%;
            padding: 12px;
            border: 2px solid #e2e8f0;
            border-radius: 8px;
            font-size: 14px;
            transition: border-color 0.3s;
            box-sizing: border-box;
        }
        
        input:focus, textarea:focus, select:focus {
            outline: none;
            border-color: #4299e1;
        }
        
        button {
            background: linear-gradient(135deg, #4299e1, #3182ce);
            color: white;
            border: none;
            padding: 12px 24px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
            transition: transform 0.2s, box-shadow 0.2s;
            margin: 5px;
        }
        
        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(66, 153, 225, 0.3);
        }
        
        button:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }
        
        button.recording {
            background: linear-gradient(135deg, #e53e3e, #c53030);
            animation: pulse 1.5s infinite;
        }
        
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }
        
        .speech-indicator {
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            border-radius: 8px;
            padding: 10px;
            margin: 10px 0;
            text-align: center;
            font-weight: 500;
            color: #856404;
        }
        
        .chat-container {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            max-height: 400px;
            overflow-y: auto;
        }
        
        .message {
            margin-bottom: 15px;
            padding: 12px;
            border-radius: 8px;
        }
        
        .user-message {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
        }
        
        .claude-message {
            background: #f3e5f5;
            border-left: 4px solid #9c27b0;
        }
        
        .coach-section {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            margin: 5px 0;
            padding: 8px;
            border-radius: 5px;
            font-style: italic;
        }
        
        .character-section {
            background: #e8f5e8;
            border-left: 4px solid #4caf50;
            margin: 5px 0;
            padding: 8px;
            border-radius: 5px;
        }
        
        .controls {
            display: flex;
            gap: 10px;
            align-items: center;
            flex-wrap: wrap;
        }
        
        .status {
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
            font-weight: 500;
        }
        
        .status.success { background: #d4e6f1; color: #1565c0; }
        .status.error { background: #ffebee; color: #c62828; }
        .status.info { background: #e8f5e8; color: #2e7d32; }
        
        .audio-controls {
            display: flex;
            align-items: center;
            gap: 10px;
            margin-top: 10px;
        }
        
        audio {
            flex: 1;
            max-width: 300px;
        }
        
        .cors-info {
            background: #e8f5e8;
            border: 1px solid #4caf50;
            border-radius: 8px;
            padding: 15px;
            margin: 20px 0;
            color: #2e7d32;
        }
        
        .proxy-selector {
            margin: 15px 0;
        }
        
        .proxy-option {
            margin: 8px 0;
            padding: 10px;
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 5px;
        }
        
        .demo-mode {
            background: #e1f5fe;
            border: 1px solid #0288d1;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
        }
        
        .voice-selector {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 8px;
            margin-top: 10px;
        }
        
        .voice-option {
            padding: 8px 12px;
            background: #f0f0f0;
            border: 1px solid #ddd;
            border-radius: 5px;
            cursor: pointer;
            text-align: center;
            transition: background-color 0.2s;
            font-size: 12px;
        }
        
        .voice-option:hover {
            background: #e0e0e0;
        }
        
        .voice-option.selected {
            background: #4299e1;
            color: white;
        }
        
        .voice-option.coach-selected {
            background: #9f7aea;
            color: white;
        }
        
        .microphone-help {
            background: #ffebee;
            border: 1px solid #f44336;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
            color: #c62828;
        }
        
        .api-help {
            background: #fff3cd;
            border: 1px solid #ffc107;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
            color: #856404;
        }
        
        .voice-section {
            background: #f8f9ff;
            border: 1px solid #d0d0ff;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
        }
        
        .connection-status {
            display: flex;
            gap: 10px;
            align-items: center;
            margin: 10px 0;
        }
        
        .status-indicator {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #ccc;
        }
        
        .status-indicator.connected {
            background: #4caf50;
            animation: pulse-green 2s infinite;
        }
        
        .status-indicator.error {
            background: #f44336;
        }
        
        @keyframes pulse-green {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }
        
        .coach-instruction {
            background: #fff9e6;
            border: 1px solid #ffc107;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
            color: #8b5000;
        }
        
        .debug-info {
            background: #f0f0f0;
            border: 1px solid #ccc;
            border-radius: 5px;
            padding: 10px;
            margin: 10px 0;
            font-family: monospace;
            font-size: 12px;
            max-height: 150px;
            overflow-y: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé≠ Claude Roleplay with Fixed Voice Detection</h1>
        
        <div class="cors-info">
            <h3>üéØ Hybrid Voice Detection System</h3>
            <p><strong>Smart contextual analysis + manual override buttons.</strong> Prioritizes authentic conversation flow while giving you control when detection fails.</p>
        </div>
        
        <div class="connection-status">
            <div class="status-indicator" id="corsStatus"></div>
            <span>CORS Proxy Status</span>
            <div class="status-indicator" id="claudeStatus"></div>
            <span>Claude API</span>
            <div class="status-indicator" id="elevenStatus"></div>
            <span>ElevenLabs API</span>
        </div>
        
        <div class="proxy-selector">
            <h3>üîß CORS Solutions</h3>
            <div class="proxy-option">
                <label>
                    <input type="radio" name="corsMethod" value="corsfix" checked> 
                    <strong>Corsfix Professional (Recommended)</strong>
                </label>
                <small>Professional service with reliable API access</small>
            </div>
            <div class="proxy-option">
                <label>
                    <input type="radio" name="corsMethod" value="demo"> 
                    <strong>Demo Mode (For Testing)</strong>
                </label>
                <small>Test interface with mock responses</small>
            </div>
        </div>
        
        <div class="demo-mode" id="demoInfo" style="display: none;">
            <h3>üéØ Demo Mode Active</h3>
            <p>This mode simulates API responses with fixed voice detection. Switch to Corsfix above to use real APIs.</p>
        </div>
        
        <div class="api-help">
            <h3>üìã API Key Instructions</h3>
            <p><strong>Claude API:</strong> Get your key from <a href="https://console.anthropic.com" target="_blank">console.anthropic.com</a> ‚Üí API Keys</p>
            <p><strong>ElevenLabs API:</strong> Get your key from <a href="https://elevenlabs.io/app/speech-synthesis" target="_blank">elevenlabs.io</a> ‚Üí Profile ‚Üí API Keys</p>
        </div>
        
        <div class="microphone-help" id="microphoneHelp" style="display: none;">
            <h3>üé§ Microphone Permission Help</h3>
            <p><strong>If speech recognition doesn't work:</strong></p>
            <ol>
                <li><strong>HTTPS Required:</strong> Upload to GitHub Pages, Netlify, or Vercel for microphone access</li>
                <li><strong>Chrome/Edge:</strong> Click the üé§ icon in address bar ‚Üí Allow microphone</li>
                <li><strong>Manual check:</strong> <button onclick="checkMicrophonePermission()" style="display: inline; padding: 5px 10px; margin: 0;">Test Microphone Access</button></li>
            </ol>
        </div>
        
        <div class="api-setup" id="apiSetup">
            <h3>API Configuration</h3>
            <div class="input-group">
                <label for="claudeKey">Claude API Key:</label>
                <input type="password" id="claudeKey" placeholder="sk-ant-... (get from console.anthropic.com)">
            </div>
            <div class="input-group">
                <label for="elevenKey">ElevenLabs API Key:</label>
                <input type="password" id="elevenKey" placeholder="Get from elevenlabs.io/app/speech-synthesis">
            </div>
            
            <div class="voice-section">
                <h4>üéØ Character Voice Selection:</h4>
                <label>Character Voice (for roleplay scenarios):</label>
                <div class="voice-selector" id="characterVoices">
                    <div class="voice-option selected" data-voice="pNInz6obpgDQGcFmaJgB">Adam</div>
                    <div class="voice-option" data-voice="21m00Tcm4TlvDq8ikWAM">Rachel</div>
                    <div class="voice-option" data-voice="AZnzlk1XvdvUeBnXmlld">Domi</div>
                    <div class="voice-option" data-voice="EXAVITQu4vr4xnSDxMaL">Bella</div>
                    <div class="voice-option" data-voice="ErXwobaYiN019PkySvjV">Antoni</div>
                    <div class="voice-option" data-voice="MF3mGyEYCl7XYWbV9V6O">Elli</div>
                </div>
                <input type="text" id="customCharacterVoice" placeholder="Or enter custom Character Voice ID" style="margin-top: 10px;">
            </div>
            
            <div class="voice-section">
                <h4>üéì Coach Voice Selection:</h4>
                <label>Coach Voice (detected automatically in responses):</label>
                <div class="voice-selector" id="coachVoices">
                    <div class="voice-option coach-selected" data-voice="21m00Tcm4TlvDq8ikWAM">Rachel</div>
                    <div class="voice-option" data-voice="pNInz6obpgDQGcFmaJgB">Adam</div>
                    <div class="voice-option" data-voice="AZnzlk1XvdvUeBnXmlld">Domi</div>
                    <div class="voice-option" data-voice="EXAVITQu4vr4xnSDxMaL">Bella</div>
                    <div class="voice-option" data-voice="ErXwobaYiN019PkySvjV">Antoni</div>
                    <div class="voice-option" data-voice="MF3mGyEYCl7XYWbV9V6O">Elli</div>
                </div>
                <input type="text" id="customCoachVoice" placeholder="Or enter custom Coach Voice ID" style="margin-top: 10px;">
            </div>
        </div>
        
        <div class="coach-instruction">
            <h4>üéØ Hybrid Voice Detection System:</h4>
            <ul>
                <li><strong>Smart Contextual Analysis:</strong> Analyzes conversation flow, emotional shifts, and teaching moments</li>
                <li><strong>Natural Coach Detection:</strong> Catches coaching language without forcing artificial markers</li>
                <li><strong>Manual Override Controls:</strong> Yellow buttons appear when you can force specific voice</li>
                <li><strong>Conversation Context:</strong> Learns from your dialogue patterns and emotional states</li>
                <li><strong>Authentic Feel:</strong> Prioritizes natural conversation flow over explicit markers</li>
                <li><strong>Backup Pattern Detection:</strong> Still catches explicit **COACH:** markers when used</li>
            </ul>
            <p><strong>How it works:</strong> The system analyzes conversation context, emotional shifts, teaching intent, and coaching activities to intelligently switch voices while keeping dialogue natural.</p>
        </div>
        
        <div class="input-group">
            <label for="scenario">Roleplay Scenario & Coach Instructions:</label>
            <textarea id="scenario" rows="4" placeholder="e.g., 'You are a friendly wizard helping me on a quest. You also act as a coach who steps in periodically to give me guidance on my roleplay choices. When coaching, ALWAYS use clear markers like **COACH:** or [Coach] or *Coach mode:* to indicate when you're stepping out of character.'"></textarea>
        </div>
        
        <div class="input-group">
            <label for="userInput">Your Message:</label>
            <textarea id="userInput" rows="2" placeholder="Type your message here..."></textarea>
        </div>
        
        <div class="controls">
            <button onclick="sendMessage()">Send & Speak</button>
            <button onclick="toggleSpeechRecognition()" id="speechBtn">üé§ Start Speaking</button>
            <button onclick="stopAudio()">Stop Audio</button>
            <button onclick="clearChat()">Clear Chat</button>
            <button onclick="testConnection()">Test APIs</button>
            <button onclick="testVoiceDetection()">Test Voice Detection</button>
            <button onclick="toggleDebug()">Toggle Debug</button>
        </div>
        
        <div class="controls" id="voiceOverrideControls" style="display: none; background: #fff3cd; padding: 10px; border-radius: 5px; margin: 10px 0;">
            <strong>Voice Override:</strong> Detection thinks this should be <span id="detectedVoice"></span> voice. Override?
            <button onclick="forceVoice('coach')" style="background: #9f7aea;">üéì Force Coach Voice</button>
            <button onclick="forceVoice('character')" style="background: #4299e1;">üé≠ Force Character Voice</button>
            <button onclick="useDetectedVoice()" style="background: #48bb78;">‚úÖ Use Detected</button>
        </div>
        
        <div id="status"></div>
        
        <div id="speechIndicator" class="speech-indicator" style="display: none;">
            üé§ Listening... Speak now, then click "Stop Speaking" when done.
        </div>
        
        <div class="debug-info" id="debugInfo" style="display: none;">
            <strong>Voice Detection Debug:</strong>
            <div id="debugContent"></div>
        </div>
        
        <div class="chat-container" id="chatContainer">
            <p style="text-align: center; color: #666;">Set up your scenario with coach instructions, then start chatting!</p>
        </div>
        
        <div class="audio-controls" id="audioControls" style="display: none;">
            <audio controls id="audioPlayer"></audio>
        </div>
    </div>

    <script>
        let currentAudio = null;
        let conversationHistory = [];
        let recognition = null;
        let isRecording = false;
        let selectedCharacterVoice = 'pNInz6obpgDQGcFmaJgB';
        let selectedCoachVoice = '21m00Tcm4TlvDq8ikWAM';
        let corsMethod = 'corsfix';
        let debugMode = false;
        let conversationTurn = 0;
        let lastUserMessage = '';
        let lastClaudeResponse = '';
        let pendingVoiceOverride = null; // For manual voice selection

        function updateStatusIndicator(element, status) {
            if (element) {
                element.className = `status-indicator ${status}`;
            }
        }

        function showStatus(message, type = 'info') {
            const status = document.getElementById('status');
            if (status) {
                status.className = `status ${type}`;
                status.textContent = message;
                setTimeout(() => status.textContent = '', 10000);
            }
        }

        function addDebugInfo(message) {
            if (debugMode) {
                const debugContent = document.getElementById('debugContent');
                if (debugContent) {
                    const timestamp = new Date().toLocaleTimeString();
                    debugContent.innerHTML += `[${timestamp}] ${message}<br>`;
                    debugContent.scrollTop = debugContent.scrollHeight;
                }
            }
            console.log('üîç DEBUG:', message);
        }

        function testVoiceDetection() {
            const testPhrases = [
                "I am a female coach! My goal is to help guide you through these tricky conversations.",
                "Hi, I'm the Coach again. You've made a good observation.",
                "**COACH:** Let me step in here to help you with this technique.",
                "[COACH] This is exactly what we need to practice.",
                "Would you like to watch that 5-minute video I mentioned?",
                "Let me help you with a better approach to this situation.",
                "Here's a technique that might be useful for you.",
                "Hi there, what you just said really ticks me off." // Should be character
            ];
            
            addDebugInfo("=== TESTING HYBRID VOICE DETECTION ===");
            
            const mockContext = {
                lastUserMessage: "I'm confused about what to do",
                conversationTurn: 5,
                recentHistory: []
            };
            
            testPhrases.forEach((phrase, i) => {
                const isCoach = hybridDetectCoachSpeaking(phrase, mockContext);
                const expected = i < 7 ? "COACH" : "CHARACTER"; // Last one should be character
                const result = isCoach ? "COACH" : "CHARACTER";
                const status = (result === expected) ? "‚úÖ CORRECT" : "‚ùå WRONG";
                
                addDebugInfo(`Test ${i+1}: ${status} - "${phrase.substring(0, 40)}..." ‚Üí ${result} (expected: ${expected})`);
            });
            
            addDebugInfo("=== CONTEXTUAL SCENARIOS ===");
            
            // Test contextual detection
            const contextualTests = [
                {
                    context: { lastUserMessage: "I'm really confused", conversationTurn: 3 },
                    response: "Let me explain this in a different way.",
                    expected: "COACH"
                },
                {
                    context: { lastUserMessage: "That made me angry", conversationTurn: 2 },
                    response: "Here's a better technique for handling anger.",
                    expected: "COACH"
                },
                {
                    context: { lastUserMessage: "Hello there", conversationTurn: 1 },
                    response: "Hey! How's your day going?",
                    expected: "CHARACTER"
                }
            ];
            
            contextualTests.forEach((test, i) => {
                const isCoach = hybridDetectCoachSpeaking(test.response, test.context);
                const result = isCoach ? "COACH" : "CHARACTER";
                const status = (result === test.expected) ? "‚úÖ CORRECT" : "‚ùå WRONG";
                
                addDebugInfo(`Context ${i+1}: ${status} - User: "${test.context.lastUserMessage}" ‚Üí Claude: "${test.response}" ‚Üí ${result}`);
            });
            
            showStatus('Hybrid voice detection test completed - check debug log', 'info');
        }

        function toggleDebug() {
            debugMode = !debugMode;
            const debugInfo = document.getElementById('debugInfo');
            if (debugInfo) {
                debugInfo.style.display = debugMode ? 'block' : 'none';
                if (debugMode) {
                    addDebugInfo('Debug mode enabled - voice detection info will appear here');
                }
            }
        }

        function addMessage(content, isUser = false) {
            const chatContainer = document.getElementById('chatContainer');
            if (!chatContainer) return;
            
            if (isUser) {
                const messageDiv = document.createElement('div');
                messageDiv.className = 'message user-message';
                messageDiv.innerHTML = `<strong>You:</strong> ${content}`;
                chatContainer.appendChild(messageDiv);
            } else {
                // Split and display coach/character sections visually
                const sections = hybridSplitCoachAndCharacter(content);
                const messageWrapper = document.createElement('div');
                messageWrapper.className = 'message claude-message';
                messageWrapper.innerHTML = '<strong>Claude:</strong>';
                
                sections.forEach(section => {
                    const sectionDiv = document.createElement('div');
                    sectionDiv.className = section.type === 'coach' ? 'coach-section' : 'character-section';
                    sectionDiv.innerHTML = `<strong>${section.type === 'coach' ? 'üéì Coach' : 'üé≠ Character'}:</strong> ${section.content}`;
                    messageWrapper.appendChild(sectionDiv);
                });
                
                chatContainer.appendChild(messageWrapper);
            }
            
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }

        // HYBRID: Smart contextual detection + explicit patterns + manual override
        function hybridDetectCoachSpeaking(text, conversationContext = null) {
            const lowerText = text.toLowerCase().trim();
            
            // PHASE 1: Explicit coach markers (for when Claude does use them)
            const explicitMarkers = [
                '**coach**', '**coach:', '[coach]', '(coach)', 
                '*coach*', 'coach:', '**coach mode**', '*coach mode*'
            ];
            
            for (let marker of explicitMarkers) {
                if (lowerText.includes(marker)) {
                    addDebugInfo(`‚úÖ EXPLICIT MARKER: "${marker}"`);
                    return true;
                }
            }
            
            // PHASE 2: Contextual Analysis - The Smart Part
            let contextScore = 0;
            let contextReasons = [];
            
            // A) EMOTIONAL SHIFT DETECTION
            if (conversationContext && conversationContext.lastUserMessage) {
                const userMsg = conversationContext.lastUserMessage.toLowerCase();
                const responseMsg = lowerText;
                
                // User expressed confusion/frustration + Claude responds with help
                if ((userMsg.includes('confused') || userMsg.includes('don\'t understand') || 
                     userMsg.includes('stuck') || userMsg.includes('help')) &&
                    (responseMsg.includes('let me') || responseMsg.includes('help you') || 
                     responseMsg.includes('explain') || responseMsg.includes('try this'))) {
                    contextScore += 3;
                    contextReasons.push('User confusion ‚Üí helpful response');
                }
                
                // Conversation escalated + Claude provides guidance
                if ((userMsg.includes('upset') || userMsg.includes('angry') || 
                     userMsg.includes('frustrated')) &&
                    (responseMsg.includes('better way') || responseMsg.includes('technique') || 
                     responseMsg.includes('approach'))) {
                    contextScore += 3;
                    contextReasons.push('Escalation ‚Üí guidance offered');
                }
            }
            
            // B) TEACHING/GUIDANCE INTENT ANALYSIS
            const teachingIndicators = [
                'let me explain', 'here\'s what', 'try this', 'better approach',
                'helpful to', 'important to understand', 'good way to',
                'technique for', 'strategy is', 'helpful if you',
                'might want to', 'consider this', 'useful to',
                'let me show you', 'way to handle'
            ];
            
            for (let indicator of teachingIndicators) {
                if (lowerText.includes(indicator)) {
                    contextScore += 2;
                    contextReasons.push(`Teaching intent: "${indicator}"`);
                }
            }
            
            // C) META-CONVERSATION DETECTION
            const metaIndicators = [
                'good observation', 'you\'ve noticed', 'important point',
                'clarify something', 'step back', 'pause here',
                'learning moment', 'opportunity to', 'practice this',
                'what just happened', 'notice how', 'reflection on'
            ];
            
            for (let meta of metaIndicators) {
                if (lowerText.includes(meta)) {
                    contextScore += 2;
                    contextReasons.push(`Meta-commentary: "${meta}"`);
                }
            }
            
            // D) DIRECT COACHING ACTIVITIES
            const coachingActivities = [
                'watch a video', 'watch that video', 'video about',
                'learn some skills', 'practice technique', 'exercise',
                'homework', 'assignment', 'try again', 'next time',
                'remember to', 'focus on', 'pay attention'
            ];
            
            for (let activity of coachingActivities) {
                if (lowerText.includes(activity)) {
                    contextScore += 3;
                    contextReasons.push(`Coaching activity: "${activity}"`);
                }
            }
            
            // E) TONE AND STRUCTURE ANALYSIS
            if (text.includes('?') && lowerText.includes('would you like')) {
                contextScore += 1;
                contextReasons.push('Offering guidance/choice');
            }
            
            if (lowerText.includes('first') || lowerText.includes('second') || 
                lowerText.includes('step 1') || lowerText.includes('next')) {
                contextScore += 1;
                contextReasons.push('Sequential instruction');
            }
            
            // PHASE 3: Obvious coach self-identification (backup for natural language)
            const coachSelfId = [
                'i\'m here to help', 'here to guide', 'my role is',
                'i want to help you', 'let me guide you', 'i can help',
                'as someone who', 'in my experience', 'i\'ve seen this'
            ];
            
            for (let selfId of coachSelfId) {
                if (lowerText.includes(selfId)) {
                    contextScore += 2;
                    contextReasons.push(`Coach self-identification: "${selfId}"`);
                }
            }
            
            // DECISION LOGIC
            const isCoach = contextScore >= 3;
            
            if (isCoach) {
                addDebugInfo(`üéØ CONTEXTUAL COACH DETECTED (score: ${contextScore}) - Reasons: ${contextReasons.join(', ')}`);
            } else {
                addDebugInfo(`üé≠ CHARACTER VOICE (score: ${contextScore}) - ${contextReasons.length > 0 ? 'Reasons: ' + contextReasons.join(', ') : 'No coaching indicators'}`);
            }
            
            return isCoach;
        }

        // HYBRID: Enhanced splitting with contextual awareness
        function hybridSplitCoachAndCharacter(text) {
            addDebugInfo(`üîç ANALYZING FULL TEXT (${text.length} chars): "${text.substring(0, 100)}..."`);
            
            // Build conversation context for smarter detection
            const conversationContext = {
                lastUserMessage: lastUserMessage,
                conversationTurn: conversationTurn,
                recentHistory: conversationHistory.slice(-4) // Last 4 exchanges
            };
            
            // Check for explicit section markers first
            const explicitSectionPattern = /(\*\*coach\*\*:|\[coach\]|\*coach\*:|\*\*coach mode\*\*:)/gi;
            
            if (explicitSectionPattern.test(text)) {
                addDebugInfo('üìç EXPLICIT SECTION MARKERS found - using marker-based splitting');
                return splitByExplicitMarkers(text);
            }
            
            // For no explicit markers, use hybrid contextual analysis
            const paragraphs = text.split(/\n\n+/);
            if (paragraphs.length > 1) {
                addDebugInfo(`üìÑ MULTIPLE PARAGRAPHS detected (${paragraphs.length}) - analyzing each with context`);
                return splitParagraphsWithContext(paragraphs, conversationContext);
            }
            
            // Single paragraph - analyze sentences with context
            const sentences = text.split(/[.!?]+(?=\s+[A-Z])/);
            if (sentences.length > 2) {
                addDebugInfo(`üìù MULTIPLE SENTENCES detected (${sentences.length}) - analyzing with context`);
                return splitSentencesWithContext(sentences, conversationContext);
            }
            
            // Single sentence/paragraph - classify as whole with full context
            const isCoach = hybridDetectCoachSpeaking(text, conversationContext);
            const result = [{ type: isCoach ? 'coach' : 'character', content: text.trim() }];
            addDebugInfo(`üéØ SINGLE SECTION classified as: ${result[0].type} (contextual analysis)`);
            return result;
        }

        function splitByExplicitMarkers(text) {
            const markers = /(\*\*coach\*\*:|\[coach\]|\*coach\*:|\*\*coach mode\*\*:|\*stepping out of character\*|\*as your coach\*)/gi;
            const parts = text.split(markers);
            const sections = [];
            let currentType = 'character'; // Default to character
            
            for (let i = 0; i < parts.length; i++) {
                const part = parts[i].trim();
                if (!part) continue;
                
                if (markers.test(part)) {
                    currentType = 'coach';
                    continue;
                }
                
                if (part.length > 0) {
                    sections.push({ type: currentType, content: part });
                    // After a coach section, return to character unless another marker found
                    if (currentType === 'coach') currentType = 'character';
                }
            }
            
            return sections.length > 0 ? sections : [{ type: 'character', content: text }];
        }

        function splitParagraphsWithContext(paragraphs, conversationContext) {
            const sections = [];
            
            for (let paragraph of paragraphs) {
                const trimmed = paragraph.trim();
                if (!trimmed) continue;
                
                const isCoach = hybridDetectCoachSpeaking(trimmed, conversationContext);
                sections.push({ 
                    type: isCoach ? 'coach' : 'character', 
                    content: trimmed 
                });
            }
            
            return sections.length > 0 ? sections : [{ type: 'character', content: paragraphs.join('\n\n') }];
        }

        function splitSentencesWithContext(sentences, conversationContext) {
            const sections = [];
            let currentSection = null;
            
            for (let sentence of sentences) {
                const trimmed = sentence.trim();
                if (!trimmed) continue;
                
                const isCoach = hybridDetectCoachSpeaking(trimmed, conversationContext);
                const type = isCoach ? 'coach' : 'character';
                
                if (!currentSection || currentSection.type !== type) {
                    if (currentSection) sections.push(currentSection);
                    currentSection = { type: type, content: trimmed };
                } else {
                    currentSection.content += '. ' + trimmed;
                }
            }
            
            if (currentSection) sections.push(currentSection);
            
            return sections.length > 0 ? sections : [{ type: 'character', content: sentences.join('. ') }];
        }

        // Manual override functions
        function forceVoice(voiceType) {
            pendingVoiceOverride = voiceType;
            const voiceOverrideControls = document.getElementById('voiceOverrideControls');
            if (voiceOverrideControls) {
                voiceOverrideControls.style.display = 'none';
            }
            
            // Replay the last response with forced voice
            if (lastClaudeResponse) {
                const sections = [{ type: voiceType, content: lastClaudeResponse }];
                showStatus(`üîß Using ${voiceType.toUpperCase()} voice (manual override)`, 'info');
                playMultipleVoiceSections(sections);
            }
            
            addDebugInfo(`üîß MANUAL OVERRIDE: Forced ${voiceType.toUpperCase()} voice`);
        }

        function useDetectedVoice() {
            const voiceOverrideControls = document.getElementById('voiceOverrideControls');
            if (voiceOverrideControls) {
                voiceOverrideControls.style.display = 'none';
            }
            
            // Replay with originally detected voice
            if (lastClaudeResponse) {
                const sections = hybridSplitCoachAndCharacter(lastClaudeResponse);
                showStatus('‚úÖ Using detected voice classification', 'info');
                playMultipleVoiceSections(sections);
            }
        }

        function showVoiceOverrideOption(detectedType) {
            const voiceOverrideControls = document.getElementById('voiceOverrideControls');
            const detectedVoiceSpan = document.getElementById('detectedVoice');
            
            if (voiceOverrideControls && detectedVoiceSpan) {
                detectedVoiceSpan.textContent = detectedType.toUpperCase();
                voiceOverrideControls.style.display = 'block';
                
                // Auto-hide after 10 seconds
                setTimeout(() => {
                    voiceOverrideControls.style.display = 'none';
                }, 10000);
            }
        }

        function initializeSpeechRecognition() {
            if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
                return false;
            }
            
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognition = new SpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';
            
            recognition.onstart = function() {
                const speechIndicator = document.getElementById('speechIndicator');
                if (speechIndicator) {
                    speechIndicator.textContent = 'üé§ Listening... Speak now!';
                    speechIndicator.style.display = 'block';
                }
                showStatus('üé§ Listening for your voice...', 'success');
            };
            
            recognition.onresult = function(event) {
                let interimTranscript = '';
                let finalTranscript = '';
                
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript;
                    } else {
                        interimTranscript += transcript;
                    }
                }
                
                if (interimTranscript) {
                    const speechIndicator = document.getElementById('speechIndicator');
                    if (speechIndicator) {
                        speechIndicator.textContent = `üé§ Hearing: "${interimTranscript}"`;
                    }
                }
                
                if (finalTranscript) {
                    const userInput = document.getElementById('userInput');
                    if (userInput) {
                        const currentText = userInput.value;
                        userInput.value = currentText + finalTranscript + ' ';
                        showStatus(`‚úÖ Added: "${finalTranscript}"`, 'success');
                    }
                }
            };
            
            recognition.onerror = function(event) {
                let errorMessage = 'Speech recognition error: ';
                switch(event.error) {
                    case 'not-allowed':
                        errorMessage += 'Microphone permission denied.';
                        const microphoneHelp = document.getElementById('microphoneHelp');
                        if (microphoneHelp) {
                            microphoneHelp.style.display = 'block';
                        }
                        break;
                    case 'no-speech':
                        errorMessage += 'No speech detected.';
                        break;
                    default:
                        errorMessage += event.error;
                }
                showStatus(errorMessage, 'error');
                stopSpeechRecognition();
            };
            
            recognition.onend = function() {
                if (isRecording) {
                    stopSpeechRecognition();
                }
            };
            
            return true;
        }

        async function toggleSpeechRecognition() {
            if (!recognition) {
                showStatus('Speech recognition not supported. Try Chrome, Edge, or Safari.', 'error');
                return;
            }
            
            if (isRecording) {
                stopSpeechRecognition();
            } else {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    stream.getTracks().forEach(track => track.stop());
                    startSpeechRecognition();
                } catch (error) {
                    if (error.name === 'NotAllowedError') {
                        showStatus('‚ùå Microphone access denied. Please allow microphone access.', 'error');
                        const microphoneHelp = document.getElementById('microphoneHelp');
                        if (microphoneHelp) {
                            microphoneHelp.style.display = 'block';
                        }
                    } else {
                        showStatus(`‚ùå Microphone error: ${error.message}`, 'error');
                    }
                }
            }
        }
        
        function startSpeechRecognition() {
            isRecording = true;
            const speechBtn = document.getElementById('speechBtn');
            const speechIndicator = document.getElementById('speechIndicator');
            
            if (speechBtn) {
                speechBtn.textContent = 'üõë Stop Speaking';
                speechBtn.classList.add('recording');
            }
            
            if (speechIndicator) {
                speechIndicator.style.display = 'block';
                speechIndicator.textContent = 'üé§ Starting microphone...';
            }
            
            try {
                recognition.start();
                showStatus('üé§ Starting speech recognition...', 'info');
            } catch (error) {
                showStatus(`Failed to start speech recognition: ${error.message}`, 'error');
                stopSpeechRecognition();
            }
        }
        
        function stopSpeechRecognition() {
            isRecording = false;
            const speechBtn = document.getElementById('speechBtn');
            const speechIndicator = document.getElementById('speechIndicator');
            
            if (speechBtn) {
                speechBtn.textContent = 'üé§ Start Speaking';
                speechBtn.classList.remove('recording');
            }
            
            if (speechIndicator) {
                speechIndicator.style.display = 'none';
            }
            
            if (recognition) {
                try {
                    recognition.stop();
                } catch (error) {
                    console.error('Error stopping recognition:', error);
                }
            }
            showStatus('Stopped listening', 'info');
        }

        async function checkMicrophonePermission() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                stream.getTracks().forEach(track => track.stop());
                showStatus('‚úÖ Microphone access granted!', 'success');
                const microphoneHelp = document.getElementById('microphoneHelp');
                if (microphoneHelp) {
                    microphoneHelp.style.display = 'none';
                }
                const speechBtn = document.getElementById('speechBtn');
                if (speechBtn) {
                    speechBtn.disabled = false;
                }
            } catch (error) {
                showStatus(`‚ùå Microphone error: ${error.message}`, 'error');
            }
        }

        async function makeAPICallWithCorsfix(url, options) {
            try {
                showStatus("Using Corsfix professional service...", "info");
                
                const response = await fetch(`https://proxy.corsfix.com/?${url}`, {
                    method: options.method || "GET",
                    headers: {
                        ...options.headers,
                        "x-corsfix-headers": JSON.stringify({
                            Origin: "",
                        }),
                    },
                    body: options.body || null,
                });

                if (!response.ok) {
                    throw new Error(`Error: ${response.status} - ${await response.text()}`);
                }

                return response;
            } catch (error) {
                showStatus(`Corsfix service error: ${error.message}`, "error");
                throw error;
            }
        }

        // FIXED: Better system prompt that enforces clearer coach/character distinctions
        async function makeClaudeRequest(userMessage, systemPrompt) {
            if (corsMethod === 'demo') {
                await new Promise(resolve => setTimeout(resolve, 1500));
                
                const scenarioContent = document.getElementById('scenario').value.trim();
                if (!scenarioContent) {
                    return "Please enter your roleplay scenario and coach instructions first!";
                }
                
                // Demo responses with clearer coach markers
                const demoResponses = [
                    "I understand what you're looking for. Let me help you with that quest. **COACH:** This is a great example of how to establish clear objectives in roleplay scenarios. Notice how I'm acknowledging your request while staying in character.",
                    "Very interesting choice! Let me see what happens next in our story. \n\n**Coach mode:** I want to pause here briefly - you're doing well with making decisive choices. This kind of commitment to your character's actions really drives the narrative forward.",
                    "That sounds like a challenging situation indeed! **COACH:** This is exactly the type of complex scenario where you can practice problem-solving skills. Consider multiple approaches before deciding."
                ];
                
                return demoResponses[conversationTurn % demoResponses.length];
            }

            const claudeKey = document.getElementById('claudeKey').value;
            if (!claudeKey) {
                throw new Error('Claude API key is required');
            }

            // HYBRID: More natural system prompt that doesn't force artificial markers
            const enhancedSystemPrompt = `${systemPrompt}

NATURAL COACHING INTEGRATION:
- You have two modes: CHARACTER (staying in roleplay) and COACH (stepping out to guide/teach)
- Switch naturally between modes based on conversation flow and learning opportunities
- When coaching, use natural language like "Let me help you with this" or "Here's a better approach"
- Only use explicit markers like "**COACH:**" if the context really calls for it
- Coach about: communication techniques, roleplay strategies, emotional skills, conversation management
- Natural coaching moments: after conflicts, when user seems stuck, during learning opportunities
- Keep the roleplay feeling authentic and flowing

Conversation context: Turn ${conversationTurn}
Last user message: "${lastUserMessage}"

Scenario: ${document.getElementById('scenario').value}`;

            const requestData = {
                model: 'claude-3-5-sonnet-20241022',
                max_tokens: 1000,
                system: enhancedSystemPrompt,
                messages: [...conversationHistory, { role: "user", content: userMessage }]
            };

            const response = await makeAPICallWithCorsfix('https://api.anthropic.com/v1/messages', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'x-api-key': claudeKey,
                    'anthropic-version': '2023-06-01'
                },
                body: JSON.stringify(requestData)
            });

            const data = await response.json();
            return data.content[0].text;
        }

        async function makeElevenLabsRequest(text, isCoach = false) {
            addDebugInfo(`VOICE REQUEST: ${isCoach ? 'COACH' : 'CHARACTER'} voice for: "${text.substring(0, 50)}..."`);
            
            if (corsMethod === 'demo') {
                // Create demo audio with different characteristics
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const duration = Math.min(text.length / 20, 3); // Variable duration based on text length
                const sampleRate = audioContext.sampleRate;
                const frameCount = duration * sampleRate;
                
                const audioBuffer = audioContext.createBuffer(1, frameCount, sampleRate);
                const channelData = audioBuffer.getChannelData(0);
                
                // Different audio patterns for coach vs character
                const frequency = isCoach ? 800 : 400;
                const pattern = isCoach ? 2 : 1; // Coach has more complex pattern
                
                for (let i = 0; i < frameCount; i++) {
                    const t = i / sampleRate;
                    channelData[i] = Math.sin(2 * Math.PI * frequency * t * pattern) * 0.1 * Math.exp(-t / duration * 2);
                }
                
                addDebugInfo(`Generated ${isCoach ? 'COACH' : 'CHARACTER'} demo audio (${duration.toFixed(1)}s)`);
                return new Blob([new ArrayBuffer(1024)], { type: 'audio/mpeg' });
            }

            const elevenKey = document.getElementById('elevenKey').value;
            if (!elevenKey) {
                throw new Error('ElevenLabs API key is required');
            }
            
            let voiceId;
            if (isCoach) {
                const customCoachVoice = document.getElementById('customCoachVoice').value;
                voiceId = customCoachVoice || selectedCoachVoice;
                addDebugInfo(`Using COACH voice: ${voiceId}`);
            } else {
                const customCharacterVoice = document.getElementById('customCharacterVoice').value;
                voiceId = customCharacterVoice || selectedCharacterVoice;
                addDebugInfo(`Using CHARACTER voice: ${voiceId}`);
            }
            
            const requestData = {
                text: text,
                model_id: "eleven_monolingual_v1",
                voice_settings: {
                    stability: isCoach ? 0.8 : 0.5,
                    similarity_boost: isCoach ? 0.9 : 0.75,
                    style: isCoach ? 0.2 : 0.0,
                    use_speaker_boost: true
                }
            };

            const response = await makeAPICallWithCorsfix(`https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`, {
                method: 'POST',
                headers: {
                    'Accept': 'audio/mpeg',
                    'Content-Type': 'application/json',
                    'xi-api-key': elevenKey
                },
                body: JSON.stringify(requestData)
            });

            return response.blob();
        }

        async function playMultipleVoiceSections(textSections) {
            addDebugInfo(`PLAYING ${textSections.length} VOICE SECTIONS`);
            
            const elevenKey = document.getElementById('elevenKey').value;
            if (!elevenKey && corsMethod !== 'demo') {
                showStatus('‚úÖ Response received! Add ElevenLabs key for speech.', 'success');
                return;
            }

            showStatus('Converting to speech with voice switching...', 'info');
            
            for (let i = 0; i < textSections.length; i++) {
                const section = textSections[i];
                const isCoach = section.type === 'coach';
                
                addDebugInfo(`Playing section ${i + 1}/${textSections.length}: ${isCoach ? 'COACH' : 'CHARACTER'}`);
                
                try {
                    const audioBlob = await makeElevenLabsRequest(section.content, isCoach);
                    
                    if (audioBlob) {
                        const audioUrl = URL.createObjectURL(audioBlob);
                        const audioPlayer = document.getElementById('audioPlayer');
                        const audioControls = document.getElementById('audioControls');
                        
                        if (audioPlayer && audioControls) {
                            audioPlayer.src = audioUrl;
                            audioControls.style.display = 'flex';
                            currentAudio = audioPlayer;
                            
                            const voiceType = isCoach ? 'COACH' : 'CHARACTER';
                            showStatus(`üéµ Playing ${voiceType} voice (${i + 1}/${textSections.length})...`, 'success');
                            
                            await new Promise((resolve) => {
                                audioPlayer.onended = resolve;
                                audioPlayer.onerror = resolve;
                                audioPlayer.play().catch(resolve);
                            });
                            
                            if (i < textSections.length - 1) {
                                await new Promise(resolve => setTimeout(resolve, 500));
                            }
                        }
                    }
                } catch (voiceError) {
                    showStatus(`Voice synthesis failed for section ${i + 1}: ${voiceError.message}`, 'error');
                    updateStatusIndicator(document.getElementById('elevenStatus'), 'error');
                }
            }
            
            showStatus('‚úÖ All voice sections complete!', 'success');
        }

        async function testConnection() {
            if (corsMethod === 'demo') {
                showStatus('‚úÖ Demo mode - all systems ready!', 'success');
                addMessage('Demo mode test: Fixed voice detection with clear coach/character separation!');
                updateStatusIndicator(document.getElementById('claudeStatus'), 'connected');
                updateStatusIndicator(document.getElementById('elevenStatus'), 'connected');
                return;
            }

            const claudeKey = document.getElementById('claudeKey').value;
            if (!claudeKey) {
                showStatus('Please enter Claude API key first', 'error');
                return;
            }
            
            showStatus('Testing API connections...', 'info');
            
            try {
                const claudeResponse = await makeClaudeRequest('Say hello and then add a coach comment using natural language', 'Test the coach detection by responding as character then coach.');
                
                if (claudeResponse) {
                    showStatus('‚úÖ Claude API working!', 'success');
                    addMessage(claudeResponse);
                    updateStatusIndicator(document.getElementById('claudeStatus'), 'connected');
                }
                
                const elevenKey = document.getElementById('elevenKey').value;
                if (elevenKey) {
                    try {
                        const sections = hybridSplitCoachAndCharacter(claudeResponse);
                        await playMultipleVoiceSections(sections);
                        showStatus('‚úÖ Both APIs working perfectly!', 'success');
                        updateStatusIndicator(document.getElementById('elevenStatus'), 'connected');
                    } catch (voiceError) {
                        showStatus(`Claude works, but ElevenLabs failed: ${voiceError.message}`, 'error');
                        updateStatusIndicator(document.getElementById('elevenStatus'), 'error');
                    }
                } else {
                    showStatus('‚úÖ Claude API working! Add ElevenLabs key for voice.', 'success');
                }
            } catch (error) {
                showStatus(`‚ùå API test failed: ${error.message}`, 'error');
                updateStatusIndicator(document.getElementById('claudeStatus'), 'error');
            }
        }

        async function sendMessage() {
            if (corsMethod !== 'demo') {
                const claudeKey = document.getElementById('claudeKey').value;
                if (!claudeKey) {
                    showStatus('Please enter Claude API key or switch to Demo mode', 'error');
                    return;
                }
            }

            const userInput = document.getElementById('userInput').value.trim();
            if (!userInput) {
                showStatus('Please enter a message', 'error');
                return;
            }

            const scenario = document.getElementById('scenario').value;
            showStatus('Sending message...', 'info');
            addMessage(userInput, true);
            
            // Store for contextual analysis
            lastUserMessage = userInput;
            conversationTurn++;
            addDebugInfo(`üìä CONVERSATION TURN: ${conversationTurn}, User said: "${userInput.substring(0, 50)}..."`);

            try {
                const systemPrompt = scenario || "You are engaged in a roleplay conversation. Also act as a coach who occasionally steps in to provide guidance. Keep responses natural and authentic - avoid forced markers unless necessary.";
                
                const claudeText = await makeClaudeRequest(userInput, systemPrompt);
                lastClaudeResponse = claudeText;
                
                addMessage(claudeText);
                
                if (corsMethod !== 'demo') {
                    conversationHistory.push({ role: "user", content: userInput });
                    conversationHistory.push({ role: "assistant", content: claudeText });
                    if (conversationHistory.length > 10) {
                        conversationHistory = conversationHistory.slice(-10);
                    }
                }

                // Use hybrid detection system
                const textSections = hybridSplitCoachAndCharacter(claudeText);
                addDebugInfo(`üé≠ FINAL SECTIONS: ${textSections.map(s => `${s.type}(${s.content.length}ch)`).join(', ')}`);
                
                // Show voice override option for single sections (most common case)
                if (textSections.length === 1) {
                    const detectedType = textSections[0].type;
                    showVoiceOverrideOption(detectedType);
                }
                
                // Play voices
                if (textSections.length > 1) {
                    showStatus(`üéµ Detected ${textSections.length} voice sections`, 'info');
                    await playMultipleVoiceSections(textSections);
                } else {
                    const isCoach = textSections[0].type === 'coach';
                    const elevenKey = document.getElementById('elevenKey').value;
                    
                    if (elevenKey || corsMethod === 'demo') {
                        const voiceType = isCoach ? 'COACH' : 'CHARACTER';
                        showStatus(`üéµ Converting to speech (${voiceType} voice)...`, 'info');
                        
                        try {
                            const audioBlob = await makeElevenLabsRequest(claudeText, isCoach);
                            
                            if (audioBlob) {
                                const audioUrl = URL.createObjectURL(audioBlob);
                                const audioPlayer = document.getElementById('audioPlayer');
                                const audioControls = document.getElementById('audioControls');
                                
                                if (audioPlayer && audioControls) {
                                    audioPlayer.src = audioUrl;
                                    audioControls.style.display = 'flex';
                                    audioPlayer.play().catch(console.error);
                                    currentAudio = audioPlayer;
                                    showStatus(`‚úÖ Playing ${voiceType} voice!`, 'success');
                                }
                            }
                        } catch (voiceError) {
                            showStatus(`Text received! Voice synthesis failed: ${voiceError.message}`, 'error');
                            updateStatusIndicator(document.getElementById('elevenStatus'), 'error');
                        }
                    } else {
                        showStatus('‚úÖ Response received! Add ElevenLabs key for speech.', 'success');
                    }
                }

                document.getElementById('userInput').value = '';
            } catch (error) {
                showStatus(`‚ùå Error: ${error.message}`, 'error');
                console.error('Full error:', error);
                updateStatusIndicator(document.getElementById('claudeStatus'), 'error');
            }
        }

        function stopAudio() {
            if (currentAudio) {
                currentAudio.pause();
                currentAudio.currentTime = 0;
                showStatus('Audio stopped', 'info');
            }
        }

        function clearChat() {
            const chatContainer = document.getElementById('chatContainer');
            if (chatContainer) {
                chatContainer.innerHTML = '<p style="text-align: center; color: #666;">Start a conversation!</p>';
            }
            conversationHistory = [];
            conversationTurn = 0;
            lastUserMessage = '';
            lastClaudeResponse = '';
            pendingVoiceOverride = null;
            
            const audioControls = document.getElementById('audioControls');
            if (audioControls) {
                audioControls.style.display = 'none';
            }
            
            const voiceOverrideControls = document.getElementById('voiceOverrideControls');
            if (voiceOverrideControls) {
                voiceOverrideControls.style.display = 'none';
            }
            
            const debugContent = document.getElementById('debugContent');
            if (debugContent) {
                debugContent.innerHTML = '';
            }
            
            showStatus('Chat cleared - hybrid system reset', 'info');
            addDebugInfo('üîÑ Chat cleared - conversation context reset');
        }

        // Initialize when page loads
        document.addEventListener('DOMContentLoaded', function() {
            const speechAvailable = initializeSpeechRecognition();
            
            // CORS method selector
            document.querySelectorAll('input[name="corsMethod"]').forEach(radio => {
                radio.addEventListener('change', function() {
                    corsMethod = this.value;
                    const demoInfo = document.getElementById('demoInfo');
                    const apiSetup = document.getElementById('apiSetup');
                    
                    if (corsMethod === 'demo') {
                        if (demoInfo) demoInfo.style.display = 'block';
                        if (apiSetup) apiSetup.style.opacity = '0.5';
                        showStatus('Demo mode activated with fixed voice detection', 'info');
                        updateStatusIndicator(document.getElementById('corsStatus'), 'connected');
                    } else {
                        if (demoInfo) demoInfo.style.display = 'none';
                        if (apiSetup) apiSetup.style.opacity = '1';
                        showStatus('Corsfix professional service selected', 'info');
                        updateStatusIndicator(document.getElementById('corsStatus'), '');
                    }
                });
            });

            // Character voice selection
            document.querySelectorAll('#characterVoices .voice-option').forEach(option => {
                option.addEventListener('click', function() {
                    document.querySelectorAll('#characterVoices .voice-option').forEach(o => o.classList.remove('selected'));
                    this.classList.add('selected');
                    selectedCharacterVoice = this.dataset.voice;
                    showStatus(`Selected character voice: ${this.textContent}`, 'info');
                });
            });

            // Coach voice selection
            document.querySelectorAll('#coachVoices .voice-option').forEach(option => {
                option.addEventListener('click', function() {
                    document.querySelectorAll('#coachVoices .voice-option').forEach(o => o.classList.remove('coach-selected'));
                    this.classList.add('coach-selected');
                    selectedCoachVoice = this.dataset.voice;
                    showStatus(`Selected coach voice: ${this.textContent}`, 'info');
                });
            });

            // Enter key handler for sending messages
            const userInput = document.getElementById('userInput');
            if (userInput) {
                userInput.addEventListener('keypress', function(e) {
                    if (e.key === 'Enter' && !e.shiftKey) {
                        e.preventDefault();
                        sendMessage();
                    }
                });
            }

            // Status message and HTTPS check
            const isSecureContext = window.isSecureContext || location.protocol === 'https:';
            if (!speechAvailable) {
                showStatus('‚ö†Ô∏è Speech recognition not available. Use Chrome, Edge, or Safari.', 'error');
                const speechBtn = document.getElementById('speechBtn');
                if (speechBtn) speechBtn.disabled = true;
            } else if (!isSecureContext) {
                showStatus('‚ö†Ô∏è Speech recognition requires HTTPS. Upload to GitHub Pages/Netlify for microphone access.', 'error');
                const speechBtn = document.getElementById('speechBtn');
                if (speechBtn) speechBtn.disabled = true;
                const microphoneHelp = document.getElementById('microphoneHelp');
                if (microphoneHelp) microphoneHelp.style.display = 'block';
            } else {
                showStatus('Ready! Fixed voice detection - should work consistently now.', 'info');
            }
            
            // Initialize status indicators
            updateStatusIndicator(document.getElementById('corsStatus'), 'connected');
        });
    </script>
</body>
</html>